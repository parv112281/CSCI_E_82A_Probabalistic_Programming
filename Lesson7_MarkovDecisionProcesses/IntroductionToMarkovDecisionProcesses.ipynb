{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Markov Decision Processes\n",
    "\n",
    "## CSCI E-82A\n",
    "### Stephen Elston\n",
    "\n",
    "In this lesson we will introduce **Markov processes**, which are a **representation** of a **memoryless state transition processes**. The diagram below shows the Markov process representation in the intelligent agent, along with the interactions with the environment.   \n",
    "\n",
    "<img src=\"img/AgentEnvironment.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Interaction of agent and environment** </center>\n",
    "\n",
    "**Markov decision processes** which are widely used in planning and optimal decision theory. The closely related **Markov reward process** is necessary for planning and optimal decision methods. We will introduce the Markov reward process here. Markov decision processes are addressed in another lesson. \n",
    "\n",
    "Suggested readings: The following reading is an optional supplement to the material presented here:\n",
    "\n",
    "Sutton and Barto, second edition, Sections 3.1, 3.2, 3.3, 3.4, 3.5 or\n",
    "Russell and Norvig, third edition, Section 7.1, or\n",
    "Kochenderfer, Section 4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Processes\n",
    "\n",
    "A first order **Markov process** is a process where the probability of a transition between a **finite set of states** only depends on the current state. In other words, first order **Markov processes have no memory of past states**. The current state has all the relevant information on the history of states.   \n",
    "\n",
    "For the transition between as a state $S_t$, at time $t$ ,to the next state $S_{t+1}$, at time $t+1$, we can express a Markov process mathematically as follows:\n",
    "\n",
    "$$p[S_{t+1}\\ |\\ S_1, \\ldots, S_t] = p[S_{t+1}\\ |\\ S_t]$$\n",
    "\n",
    "For a vector of possible states, $S$, we can create a **state transition probability matrix**. This matrix **represents** the probability of a state transition from $S$ to the next state, $S'$ at the next time step:\n",
    "\n",
    "$$\\mathcal{P_{ss'}} = \n",
    "\\begin{bmatrix}\n",
    "    P_{11} & \\dots  & P_{1n} \\\\\n",
    "    \\vdots & \\vdots  & \\vdots \\\\\n",
    "    P_{n1} & \\dots  & P_{nn}\n",
    "\\end{bmatrix}\\\\\n",
    "$$\n",
    "Where, $\\mathcal{P}_{ij} =$ probability of transition from state $s_i$ to $s_j$.   \n",
    "\n",
    "Let's say we have a vector of probabilities of being in one of n possible states, $S = (s_1, s_2, \\ldots, s_n)$. Using simple matrix multiplication we can write the relationships for the transition to the next state $S'$ as:\n",
    "\n",
    "$$S' = \\mathcal{P_{ss'}} S\\\\\n",
    "or\\\\\n",
    "\\begin{bmatrix}\n",
    "    s_1' \\\\\n",
    "    \\vdots \\\\\n",
    "    s_n'\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    P_{11} & \\dots  & P_{1n} \\\\\n",
    "    \\vdots & \\vdots  & \\vdots \\\\\n",
    "    P_{n1} & \\dots  & P_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    s_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    s_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "A **Markov chain** is a sequence of of states of a Markov process. In other words, if we run a Markov process over a number of time steps the result is a Markov chain. \n",
    "\n",
    "If the transition probability matrix, $ \\mathcal{P_{ss'}}$, does not change over time, we say the Markov chain is **stationary**. Stationary Markov chains have a **convergence property**. If we run the Markov chain for enough time steps, the chain will reach a **steady state**. At steady state the probabilities of being in any state of the Markov process is **unchanged from time step to time step**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Example - Does Steve Need a New Car?\n",
    "\n",
    "Let's try a computational example to test out the foregoing concepts. In this case we will look at the state transitions for the use of an old car vs. a new car. The diagram below shows the states of car ownership and the possible transitions between them.   \n",
    "\n",
    "<img src=\"img/CarStates.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> States and possible transitions of car use </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states and the possible transitions are:\n",
    "1. Old car, can transition to continue driving the old car, a breakdown, or an accident.\n",
    "2. Old car breakdown, can transition to old car or new car.\n",
    "3. Old car accident, transitions to new car.\n",
    "4. New car, can transition to continue driving the new car, a breakdown, or an accident.\n",
    "5. New car breakdown, transitions to new car or to an old car.\n",
    "6. New car accident, transitions to new car.\n",
    "\n",
    "Notice that there are no **terminal states** in this diagram. A terminal state can be entered, but there is no possible transition to another state. An example of a terminal state is the win or loss of game. The game is over, and there will be no more states for playing. Markov processes with terminal states are said to be **periodic** or **finite** since they run for a finite period, after which they must be restarted. Whereas, Markov processes with no terminal state are said to be **infinite**, since in theory they will run for an infinite number of time steps.\n",
    "\n",
    "Given these transitions, the question is what is the probability that Steve will end up in a new car or keep his old car. \n",
    "\n",
    "We will start by defining a transition probability matrix and testing that the probabilities in the columns add to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "T = np.array([[9.8999e-01, 9.90000e-01, 0.00000e-00, 1.00000e-02, 0.00000e+00],\n",
    " [1.00000e-02, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00],\n",
    " [0.0000,1.00000e-02, 9.98999e-01, 9.90000e-01, 1.00000e+00],\n",
    " [0.00000e+00, 0.00000e+00, 1.00000e-03, 0.00000e+00, 0.00000e+00],\n",
    " [0.00001e+00, 0.00000e+00, 1.00000e-06, 0.00000e+00, 0.00000e+00]])\n",
    "\n",
    "print('The transition probability matrix')\n",
    "labels = ['OldCar','OldBreak','NewCar','NewBreak','Acident']\n",
    "print(pd.DataFrame(T, columns = labels, index = labels))\n",
    "\n",
    "print('\\nTest that the columns add to 1')\n",
    "np.sum(T, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define an initial state for the Markov chain. In this case, Steve is driving his old car, so the state vector is defined as shown below. The multiplication by the transition probability matrix gives gives the new states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = np.array([1.0,0.0, 0.0, 0.0, 0.0 ])\n",
    "pd.Series(np.matmul(T,initial_state), index = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how several states now have non-zero probabilities. Two states still have zero probabilities. \n",
    "\n",
    "Let's see what happens when we apply a series of state transitions to this initial state. The code in the cell below performs these multiplications to compute the probabilities of the new state given the current state. The function returns a list containing the state vector following each transition. The final state vector is printed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(T, s, n = 1):\n",
    "    s_list = np.reshape(s, (s.shape[0],1))\n",
    "    for _ in range(n):\n",
    "        s = np.matmul(T,s)\n",
    "        s_list = np.concatenate((s_list, np.reshape(s, (s.shape[0],1))), axis = 1)\n",
    "    return s_list\n",
    "states = state_transition(T, initial_state, 100000)\n",
    "pd.Series(states[:,999], index = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the probabilities of being in each state after a large number of transitions.\n",
    "\n",
    "Let's make a plot of two of these states. The state that Steve drives his own car or the state that he buys a new car are plotted in the chart before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_states(states, variables = [0,2]):\n",
    "    fig = plt.figure(figsize=(6,6)) # define plot area\n",
    "    ax = fig.gca() # define axis \n",
    "    ax.set_xlabel('Number of time steps')\n",
    "    ax.set_ylabel('Probability of being in state')\n",
    "    ax.set_title('Probability of states vs. number of time steps')\n",
    "    for var in variables:\n",
    "        plt.plot(states[var,:])\n",
    "        \n",
    "plot_states(states)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after about 3,000 transitions the state probabilities the probabilities remain essentially unchanged. This indicates that the Markov chain is in steady state. It seems that in the steady state, Steve does eventually end up with a new car!\n",
    "\n",
    "Let's plot the state probabilities for two other states, the probabilities of a breakdown for the old and new cars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_states(states, [1,3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before these state probabilities are in steady state after about 3,000 steps. The probability of the old car breaking down approaches zero since there is a low probability of driving the old car in the steady state.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "We can define a **reward function**, $\\mathcal{R}$ as the expected reward in the next time step as:\n",
    "\n",
    "$$\\mathcal{R}_{s s'} = E \\big[ R_{t+1}\\ |\\ S_t = s \\big]$$   \n",
    "\n",
    "Where, $R_{t+1}$ is the reward, or change in utility, for the time step, given a transition to a new state $s'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a Markov reward process. The diagram below shows the rewards for the various state transitions in the auto example. Since owning cars has significant costs, all of the rewards are negative. \n",
    "\n",
    "<img src=\"img/CarRewards.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Rewards for state transitions of car use** </center>\n",
    "\n",
    "Keep in mind that just like utility, **reward need not simply follow economic value**; e.g. money. For example, the reward of the car breaking down must account for the inconvenience of dealing with the repair, or the reward for driving the car must account for intangibles like comfort and safety of the passengers.  \n",
    "\n",
    "What is the relationship between utility and reward in a Markov chain? It is easy to compute utility from rewards, since **rewards are additive**. First, let's consider a finite Markov reward process, which reaches a terminal state after $T$ time steps:\n",
    "\n",
    "$$U([s_o, s_1, \\ldots, s_T]) = R(s_o) + R(s_1) + \\ldots + R(s_T) = \\sum_{t = 0}^T R(s_t)$$\n",
    "\n",
    "But, consider what happens with an infinite Markov reward process, which never reaches a terminal state. If we use the above formulation, the utility will grow without bound; e.g. $U(s_t) \\rightarrow \\infty$ as $T \\rightarrow \\infty$. \n",
    "\n",
    "The solution to keeping utility bounded for infinite Markov reward processes is **discounting**. By discounting we are saying that the value of a reward in the future decreases the further in the future the reward is received. This is a commonly used concept in many fields. For example, an investor will discount expected future returns, preferring immediate payoff. \n",
    "\n",
    "Using discounting, we can formulate a bounded relationship between utility and reward:\n",
    "\n",
    "$$U([s_o, s_1, s_2, s_3 \\ldots]) = R(s_o) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\gamma^3 R(s_3) \\ldots = \\sum_{t = 0}^{\\infty} \\gamma^{t} R(s_t)$$\n",
    "\n",
    "The choice of the discount parameter, $\\gamma$, will change the outcome for the Markov reward process:\n",
    "- As $\\gamma \\rightarrow 0$, the reward process becomes myopic, only counting near term rewards.\n",
    "- As $\\gamma \\rightarrow 1$, the reward process becomes far sighted, valuing distant rewards highly. \n",
    "\n",
    "For infinite Markov reward processes we are interested in the **return** for state transitions starting with the current state. Return is the sum of the rewards for future state transitions and can be expressed as:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\ldots = \\sum_{k = 0}^{\\infty} \\gamma^{k} R_{t+k+1}$$\n",
    "\n",
    "We are also interested in the **state value function**. This expression computes the expected future value of being in state $s$:\n",
    "\n",
    "$$v(s) = E[G_t\\ |\\ S_t = s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "The agent uses a **policy**, $\\pi$, to determine which action to take. The expected **action value** given the action, $a$, from state, $s$, by the policy is:   \n",
    "\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi} [G_{t}\\ |\\ S_t = s, A_t = a] $$\n",
    "\n",
    "Our goal is to find an **optimal policy** which maximizes the expected action value. We say that the optimal policy, $q_*(s,a)$, gives the highest expected value for the action $a$:\n",
    "\n",
    "$$q_{\\pi^*}(s,a) = \\mathbb{E}_{\\pi^*} [G_{t}\\ |\\ S_t = s, A_t = a] $$\n",
    "\n",
    "An optimal policy has an expected action value greater than or equal to all possible policies:\n",
    "\n",
    "$$q_{\\pi^*}(s,a) \\ge q_{\\pi}(s,a)\\ \\forall\\ \\pi$$\n",
    "\n",
    "The bandit model is stateless. Thus, there is no state required for the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Example\n",
    "\n",
    "With the above theory in mind, let's try a computational example for a Markov reward process. We are particularly interested in the convergence properties of the state value function. This convergence is key if we wish to find an optimal state for a Markov process. \n",
    "\n",
    "As a first step we, must define a matrix of the rewards for transitions between the states. Execute the code in the cell below and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Markov reward matrix\n",
    "R = np.array([[-30.0, -600, 0.0, -3000, 0.0],\n",
    " [-200.0, 0.0, 0.0, 0.0, 0.0],\n",
    " [0.0, -30000, -20, -100, -30000],\n",
    " [0.0, 0.0, -1000, 0.0, 0.0],\n",
    " [-100000, 0.0, -50000, 0.0, 0.0]])\n",
    "\n",
    "print('The reward matrix for state transitions')\n",
    "labels = ['OldCar','OldBreak','NewCar','NewBreak','Acident']\n",
    "print(pd.DataFrame(R, columns = labels, index = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes the value of being in some initial state $s$. The code comments explain the steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value(T, R, s, n = 1, gamma = 0.9):\n",
    "    v_list = [] # a list to hold the values\n",
    "    for _ in range(n):\n",
    "        s_prime = np.matmul(T,s) # The probabilities of being in the new states\n",
    "        delta_s = np.subtract(s,s_prime) # The change in probabilities of the states\n",
    "        s = s_prime\n",
    "        v_list.append(np.sum(np.matmul(R,delta_s))) # Build the list of values\n",
    "    \n",
    "    state_value = 0.0\n",
    "    ## Now loop over the state transitions and compute the discounted value\n",
    "    for i in range(n):\n",
    "        state_value = state_value + gamma**i * v_list[i]\n",
    "    return state_value\n",
    "\n",
    "compute_state_value(T, R, initial_state, n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test result looks reasonable, but what about convergence? The code in the cell below computes the discounted value of being in an initial state for an increasing number of time steps and then plots the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "def converge_state_value(T, R, s, n = 1, gamma = 0.9):\n",
    "    ## return a list for the value after each number of time steps\n",
    "    return [compute_state_value(T, R, s, steps, gamma) for steps in range(n)]\n",
    "\n",
    "values = converge_state_value(T, R, initial_state, n = 200, gamma = gamma)\n",
    "\n",
    "def plot_values(vals, gamma):\n",
    "    fig = plt.figure(figsize=(6,6)) # define plot area\n",
    "    ax = fig.gca() # define axis \n",
    "    ax.set_xlabel('Number of time steps')\n",
    "    ax.set_ylabel('value of state')\n",
    "    ax.set_title('Value of state vs. number of time steps with gamma = ' + str(gamma))\n",
    "    plt.plot(vals)\n",
    "        \n",
    "plot_values(values, gamma)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a value of $\\gamma = 0.9$ the state value function converges rather quickly, in less than 100 time steps. No doubt, the fact that the Markov process reaches steady state quickly helps. \n",
    "\n",
    "Let's see what happens for $\\gamma = 0.99$. How does this change in discounting change the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "values = converge_state_value(T, R, initial_state, n = 1000, gamma = gamma)\n",
    "plot_values(values, gamma)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for this rather low discounting the state value function converges in about 600 time steps. This result is encouraging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
