{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Latent Variable Models and Variational Methods\n",
    "\n",
    "## CSCI E-82A\n",
    "### Stephen Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Models\n",
    "\n",
    "We refer to a probabilistic model with **hidden variables** a **latent variable model**. A latent variable model has three components:  \n",
    "\n",
    "- **Visible or observed variables, $\\nu$:** Data can be acquired for these variables from **emissions** of the values.\n",
    "- **Hidden, unobserved, or latent variables, $h$:** The actual value of these variables is not observable and can only be estimated. \n",
    "- **Model parameters, $\\theta$:** Are a vector of parameters which must be estimated for the model. \n",
    "\n",
    "In general, our goal is to find the joint distribution:   \n",
    "\n",
    "$$p(\\nu, h; \\Theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Models\n",
    "\n",
    "A mixture model allows us to represent complex probability distributions. There are many real-world cases where a single distribution would not be an accurate representation. For example:\n",
    "\n",
    "- Missing value problems may require a mixture of distributions. \n",
    "- An unscrupulous casino may alternate between using fair and 'loaded' dice. The distribution of numbers shown by these two types of dice are quite different. An observer trying to model the full distribution will need to use a mixture of the two.  \n",
    "- Returns of many financial assets are dependent on overall market conditions. These returns might represent a specific log-normal distribution for a period of time, and then once investor sentiment changes, a different distribution.   \n",
    "- Response rates to a promotional email offer might represent distributions for several populations. The offer might be for men's running shoes. One population of respondent is expected to be male athletes. However, the are other potential buyers who might be purchasing the shoes on behalf of a male athlete. There response rates of these populations could be quite different, and from just an email address there is no way to know which population each response comes from. \n",
    "\n",
    "Let, $\\nu_i$ be a real-valued vector of observed values in $\\mathbb{R}^d$, and $h_i \\in \\{1, 2, 3, \\ldots, K \\}$ be a discrete-valued hidden variable. We can represent a factorized DAG of the mixture model as: \n",
    "\n",
    "$$p(\\nu, h) = p(\\nu\\ |\\ h) p(h)$$\n",
    "\n",
    "Where, $p(h = k) = \\pi_k$, for some probabilities of each of the $K$ components of the mixture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "One of the most widely used mixture models is the **mixture of Gaussian distributions**. GMMs are used in many applications from engineering, medicine, and robot navigation. \n",
    "\n",
    "As the name implies the GMM is a mixture of *K* individual Gaussian distributions where the probability of the kth distribution is $\\pi_k \\in \\{\\pi_1, \\pi_2, \\ldots, \\pi_K \\}$. Each of the *K* distributions has a location parameter, $\\mu_k \\in \\{\\mu_1, \\mu_2, \\ldots, \\mu_K\\}$ and a covariance parameter, $\\Sigma_k \\in \\{\\Sigma_1, \\Sigma_2, \\ldots, \\Sigma_K\\}$. The parameter vector for one component of the latent variable model is then:  \n",
    "\n",
    "$$\\theta_k = (\\mu_k, \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "The conditional probability distribution for a single component of the GMM can then be written:   \n",
    "\n",
    "$$p(\\nu\\ |\\ h = k) = \\mathcal{N}(\\nu; \\mu_k, \\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we actually observe is the marginal distribution of the visible variables. For the GMM we can find this marginal distribution as follows:  \n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} p(\\nu\\ |\\ h = k) p(h = k) =  \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\nu; \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Here, the hidden variable is marginalized out. The right hand term is the expectation of $p(x)$ for the mixture of Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes \n",
    "\n",
    "We need a way to perform inference to find the parameter vector, $\\theta$ of the latent variable model, $p(\\nu, h; \\Theta)$. As has already been stated, there are no exact inference methods for latent variable problems. However, there are practical approximate methods, which often work well.     \n",
    "\n",
    "The Monte Carlo method has been applied to latent variable problems for decades. More recently **variational methods** have been gaining popularity. There are several key differences between Monte Carlo methods and variational methods:    \n",
    "\n",
    "- Variational methods are **computationally more efficient** than Monte Carlo methods. This fact, has lead to the growth in the use of variational methods.\n",
    "- We can always know when a variational approximation method has converged. This is not the case with Monte Carlo methods. \n",
    "- Variational methods use **local optimization** and there is no guarantee the **global optimum** can ever be found. Whereas, Monte Carlo methods will generally find the globally optimal solution, eventually. This local convergence property is the price we pay for the efficiency of variational methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Kullback-Leibler Divergence\n",
    "\n",
    "Variational methods are based on the Kullback-Leibler divergence. Let's review some of the properties of the KL divergence.   \n",
    "\n",
    "The KL divergence between two distributions, $p(x)$ and $q(x)$ is written:\n",
    "\n",
    "$$\\mathbb{D}_{KL}(P \\parallel Q) = - \\sum_{x} p(x)\\ ln_b \\frac{p(x)}{q(x)}$$   \n",
    "\n",
    "Some key properties of the KL divergence include:   \n",
    "\n",
    "- $\\mathbb{D}_{KL}(P \\parallel Q) \\ge 0$ for all $p(x)$ and $q(x)$.\n",
    "- $\\mathbb{D}_{KL}(P \\parallel Q) = 0$ if and only if $p(x)= q(x)$.\n",
    "- KL divergence is not symmetric so, $\\mathbb{D}_{KL}(P \\parallel Q) \\ne \\mathbb{D}_{KL}(Q \\parallel P)$. This is why the term **divergence** is applied and this quantity cannot be considered a distance metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Variational Lower Bound\n",
    "\n",
    "Our problem is the find the full vector of parameters, $\\theta$, using just the data from the visible variables, $\\nu$. The problem of finding the posterior distribution of $\\theta$ given $\\nu$ can be formulated as:   \n",
    "\n",
    "$$p(\\theta\\ |\\ \\nu) \\propto p(\\nu\\ |\\ \\theta) p(\\theta) \\propto \\sum_h p(\\nu, h\\ |\\ \\theta) p(\\theta)$$  \n",
    "\n",
    "Where, $p(\\theta)$ is the prior distribution of $\\theta$. Since our goal is to find the value of $\\theta$ that maximizes the likelihood, $p(\\theta\\ |\\ \\nu)$, we can work with proportional relationships and therefore not have to deal with the troublesome normalization $Z(\\theta)$.\n",
    "\n",
    "The variational approximation assumes that the joint conditional distribution can be factorized as follows:  \n",
    "\n",
    "$$p(\\nu, h\\ |\\ \\theta) \\approx q(h) q(\\theta)$$\n",
    "\n",
    "The variational approximation is achieved by finding a value of $\\theta$ that minimizes the KL divergence between $p(h \\theta\\ |\\ \\nu)$ and $q(h) q(\\theta)$. Using these terms, the expanded KL divergence, and the properties stated in the previous section we find:  \n",
    "\n",
    "$$\\mathbb{D}_{KL}(q(h) q(\\theta) \\parallel p(h \\theta\\ |\\ \\nu)) = \n",
    "\\mathbb{E}_{q(h)} \\big[ log(q(h)) \\big] + \n",
    "\\mathbb{E}_{q(\\theta)} \\big[ log(q(\\theta)) \\big] -\n",
    "\\mathbb{E}_{q(h) q(\\theta)} \\big[ log(p(h, \\nu, \\theta)) \\big]\n",
    "\\ge 0$$\n",
    "\n",
    "Rearranging these terms we find the bound on $log(p(\\nu))$:\n",
    "\n",
    "$$log(p(\\nu)) \\ge \n",
    "-\\mathbb{E}_{q(h)} \\big[ log(q(h)) \\big] -\n",
    "\\mathbb{E}_{q(\\theta)} \\big[ log(q(\\theta)) \\big] +\n",
    "\\mathbb{E}_{q(h) q(\\theta)} \\big[ log(p(h, \\nu, \\theta)) \\big]$$  \n",
    "\n",
    "From the above, you can see that we can maximize the likelihood of the joint distribution by minimizing the KL divergence with respect to $q(h)$ and $q(\\theta)$. Given the aforementioned factorization, this minimization can be achieved coordinate-wise, making the problem tractable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in the Variational Algorithm\n",
    "\n",
    "There are two alternating steps in the variational algorithm.  \n",
    "\n",
    "- We would like to maximize the likelihood of $q(h)$. But, by definition, we cannot know the actual values of the hidden variables, $h$. However, we can compute an updated estimate, $q^{new}(h)$, using the values of observed data, $\\nu$ and current estimates of $q(\\theta)$ and $q^{old}(h)$. This process is often referred to as **hallucinating data**, since data for hidden variables is manufactured. This process is also known as the **E-step**, since it maximizes the expected value of $q(h)$.  \n",
    "  \n",
    "- Likewise, using the values of observed data, $\\nu$ and current estimates of $q^{old}(\\theta)$ and $q(h)$ we can compute an updated estimate of $q^{new}(\\theta)$. This process is also known as the **M-Step**, since it maximizes the likelihood of $q(\\theta)$.  \n",
    "\n",
    "**The E-step:**   \n",
    "\n",
    "\n",
    "\n",
    "**The M-step:**   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
