{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Assignment\n",
    "## Cliff Walking with Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    ">**Make sure** you include your name along with the name of your team and team members in the notebook you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name and team name here: *Horse* Parvinderjit Singh, Noah Peart, Lei Yue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this challenge you will apply Monte Carlo reinforcement learning algorithms to a classic problem in reinforcement learning, known as the **cliff walking problem**. The cliff walking problem is a type of game. The goal is for the agent to find the highest reward (lowest cost) path from a starting state to the goal.   \n",
    "\n",
    "There are a number of versions of the cliff walking problems which have been used as research benchmarks over the years. You can find a short discussion of the cliff walking problem on page 132 of Sutton and Barto, second edition.    \n",
    "\n",
    "In the general cliff walking problem the agent starts in one corner of the state-space and must travel to goal, or terminal state, in another corner of the state-space. Between the starting state and goal state there is an area with a **cliff**. If the agent falls off a cliff it is sent back to the starting state. A schematic diagram of the state-space is shown in the diagram below.      \n",
    "\n",
    "<img src=\"CliffWalkingDiagram.JPG\" alt=\"Drawing\" style=\"width:500px; height:400px\"/>\n",
    "<center> State-space of cliff-walking problem </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The agent must learn a policy to navigate from the starting state to the terminal state. The properties this problem are as follows:\n",
    "\n",
    "1. The state-space has two **continuous variables**, x and y.\n",
    "2. The starting state is at $x = 0.0$, $y = 0.0$. \n",
    "3. The terminal state has two segments:\n",
    "  - At $y = 0.0$ is in the range $9.0 \\le x \\le 10.0$. \n",
    "  - At $x = 10.0$ is in the range $0.0 \\le y \\le 1.0$.  \n",
    "4. The cliff zone is bounded by:\n",
    "  - $0.0 \\le y \\le 1.0$ and \n",
    "  - $1.0 \\le x \\le 9.0$. \n",
    "5. An agent entering the cliff zone is returned to the starting state.\n",
    "6. The agent moves 1.0 units per time step. \n",
    "7. The 8 possible **discrete actions** are moves in the following directions:  \n",
    "  - +x, \n",
    "  - +x, +y,\n",
    "  - +y\n",
    "  - -x, +y,\n",
    "  - -y,\n",
    "  - -x, -y,\n",
    "  - -y, and\n",
    "  - +x, -y. \n",
    "8. The rewards are:\n",
    "  - -1 for a time step in the state-space,\n",
    "  - -10 for colliding with an edge (barrier) of the state-space,\n",
    "  - -100 for falling off the cliff and returning to the starting state, and \n",
    "  - +1000 for reaching the terminal or goal state. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this challenge you and your team will do the following. Include commentary on each component of your algorithms. Make sure you answer the questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Simulator   \n",
    "\n",
    "Your reinforcement learning agent cannot contain any information about the environment other that the starting state and the possible actions. Therefore, you must create an environment simulator, with the following input and output:\n",
    "- Input: Arguments of state, the $(x,y)$ tuple, and discrete action\n",
    "- Output: the new state (s'), reward, and if the new state meets the terminal or goal criteria.\n",
    "\n",
    "Make sure you test your simulator functions carefully. The test cases must include, steps with each of the actions, falling off the cliff from each edge, hitting the barriers, and reaching the goal (terminal) edges. Errors in the simulator will make the rest of this challenge difficult.   \n",
    "\n",
    "> **Note**: For this problem, coordinate state is represented by a tuple of continuous variables. Make sure that you maintain coordinate state as continuous variables for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_environment(x, y, action):\n",
    "    # calculate new location\n",
    "    x_prime = x\n",
    "    y_prime = y\n",
    "    if (action[0] == 1):\n",
    "        if (action[1] == 1):\n",
    "            x_prime = x + .707\n",
    "            y_prime = y + .707\n",
    "        elif(action[1] == -1):\n",
    "            x_prime = x + .707\n",
    "            y_prime = y - .707\n",
    "        else:\n",
    "            X_prime = x + 1\n",
    "    elif (action[0] == -1):\n",
    "        if (action[1] == 1):\n",
    "            x_prime = x - .707\n",
    "            y_prime = y + .707\n",
    "        elif(action[1] == -1):\n",
    "            x_prime = x - .707\n",
    "            y_prime = y - .707\n",
    "        else:\n",
    "            X_prime = x - 1\n",
    "    else:\n",
    "        if (action[1] == 1):\n",
    "            y_prime = y + 1\n",
    "        elif (action[1] == -1):\n",
    "            y_prime = y - 1\n",
    "             \n",
    "    #ensure new location is in bounds\n",
    "    if (x_prime >= 10.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0\n",
    "        hit_wall = True\n",
    "    elif (x_prime <= 0.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0\n",
    "        hit_wall = True\n",
    "    if (y_prime >= 10.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0\n",
    "        hit_wall = True\n",
    "    elif (y_prime <= 0.0):\n",
    "        x_prime = 0.0\n",
    "        y_prime = 0.0  \n",
    "        hit_wall = True\n",
    "             \n",
    "    #ensure new location is not off cliff\n",
    "    if (x_prime >= 1.0 and x_prime <= 9.0):\n",
    "        if (y_prime >= 0.0 and y_prime <= 1.0):\n",
    "             x_prime = 0.0\n",
    "             y_prime = 0.0\n",
    "             fell = True\n",
    "    \n",
    "    ## At the terminal state or not and set reward\n",
    "    if (in_terminal(x_prime, y_prime)):\n",
    "        done = True\n",
    "        reward = 1000\n",
    "    else if (fell):\n",
    "        done = False\n",
    "        reward = -100\n",
    "    else if (hit_wall):\n",
    "        done = False\n",
    "        reward = -10\n",
    "    else:\n",
    "        done = False\n",
    "        reward = -1.0\n",
    "    # output new state (s'), reward, and if the new state meets the terminal or goal criteria\n",
    "    return(x_prime, y_prime, done, reward)\n",
    "\n",
    "def in_terminal(x, y):\n",
    "    if (y = 0.0 and x >= 9.0):\n",
    "        return True\n",
    "    elif (x = 10.0 and y <= 1.0):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Approximation\n",
    "\n",
    "The state-space of the cliff walking problem is continuous. Therefor, you will need to use a **grid approximation** to construct a policy. The policy is specified as the probability of action for each grid cell. For this problem, use a 10x10 grid. \n",
    "\n",
    "> **Note:** While the policy uses a grid approximation, state should be represented as continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Policy\n",
    "\n",
    "Start with a uniform initial policy. A uniform policy has an equal probability of taking any of the 8 possible actions for each cell in the grid representation.     \n",
    "\n",
    "> **Note:** As has already been stated, the coordinate state representation for this problem is a tuple of coordinate values. However, policy, state-values and action-values are represented with a grid approximation. \n",
    "\n",
    "> **Hint:** You may wish to use a 3-dimensional numpy array to code the policy for this problem. With 8 possible actions, this approach will be easier to work with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo State Value Estimation   \n",
    "\n",
    "For the initial uniform policy, compute the state values using the Monte Carlo RL algorithm:\n",
    "1. Compute and print the state values for each grid in the representation. Use at least 1,000 episodes. This will take some time to execute.      \n",
    "2. Plot the grid of state values, as an image (e.g. matplotlib [imshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html)). \n",
    "3. Compute the Forbenious norm (Euclidean norm) of the state value array with [numpy.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html). You will use this figure as a basis to compare your improved policy. \n",
    "\n",
    "Study your plot to ensure your state values seem correct. Do these state values seem reasonable given the uniform policy and why? Make sure you pay attention to the state values of the cliff zone.    \n",
    "\n",
    "> **Hint:** Careful testing at each stage of your algorithm development will potentially save you considerable time. Test your function(s) to for a single episode to make sure your algorithm converges. Then test for say 10 episodes to ensure the state values update in a reasonable manner at each episode.    \n",
    "\n",
    "> **Note:** The Monte Carlo episodes can be executed in parallel for production systems. The Markov chain of each episode is statistically independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo State Policy Improvement   \n",
    "\n",
    "Finally, you will perform Monte Carlo RL policy improvement:\n",
    "1. Starting with the uniform policy, compute action-values for each grid in the representation. Use at least 1,000 episodes.      \n",
    "2. Use these action values to find an improved policy.\n",
    "3. To evaluate your updated policy compute the state-values for this policy.  \n",
    "4. Plot the grid of state values for the improved policy, as an image. \n",
    "5. Compute the Forbenious norm (Euclidean norm) of the state value array. \n",
    "\n",
    "Compare the state value plot for the improved policy to the one for the initial uniform policy. Does the improved state values increase generally as distance to the terminal states decreases?  Is this what you expect and why?    \n",
    "\n",
    "Compare the norm of the state values with your improved policy to the norm for the uniform policy. Is the increase significant?  \n",
    "\n",
    "> **Hint:** Careful testing at each stage of your algorithm development will potentially save you considerable time. Test your function(s) to for a single episode to make sure your algorithm converges. Then test for say 10 episodes to ensure the state values update in a reasonable manner at each episode.   \n",
    "\n",
    "> **Note:** You could continue to improve policy using the general policy improvement algorithm (GPI). In the interest of time, you are not required to do so here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:\n",
    "\n",
    "ANS:\n",
    "\n",
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Create cells below for your solution to the stated problem. Be sure to include some Markdown text and code comments to explain each component of your algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
