{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "\n",
    "In the Dynamic Programming (DP) lesson Jupyter notebook, we constructed a representation of a simple grid world. DP was used to find optimal plans for a robot to navigate from any starting location on the grid to the goal. This problem is an analog for more complex real-world robot navigation problems. \n",
    "\n",
    "In this homework you will use DP to solve a slightly more complex robotic navigation problem in a grid world. This grid world is a simple version of the problem a material transport robot might encounter in a warehouse. The situation is illustrated in the figure below.\n",
    "\n",
    "<img src=\"GridWorldFactory.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> Grid World for Factory Navigation Example </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is for the robot to deliver some material to position (state) 12, shown in blue. Since there is a goal state or **terminal state** this an **episodic task**. \n",
    "\n",
    "There are some barriers comprised of the states $\\{ 6, 7, 8 \\}$ and $\\{ 16, 17, 18 \\}$, shown with hash marks. In a real warehouse, these positions might be occupied by shelving or equipment. We do not want the robot to hit these barriers. Thus, we say that transitioning to these barrier states is **taboo**.\n",
    "\n",
    "As before, we do not want the robot to hit the edges of the grid world, which represent the outer walls of the warehouse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "As with many such problems, the starting place is creating the **representation**. In the cell below encode your representation for the possible action-state transitions. From each state there are 4 possible actions:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "There are a few special cases you need to consider:\n",
    "- Any action transitioning to a state off the grid or into a barrier should keep the state unchanged. \n",
    "- Once in the goal state there are no more state transitions. \n",
    "- Any transition within the barrier (taboo) states can keep the state unchanged. If you experiment, you will see that other encodings work as well since the value of a barrier states are always zero and there are no actions transitioning into these states. \n",
    "\n",
    "> **Hint:** It may help you create a pencil and paper sketch of the transitions, rewards, and probabilities or policy. This can help you to keep the bookkeeping correct. \n",
    "\n",
    "In the cell below define a dictionary where your code can look up the successor state given the current state and action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define the initial policy for the Markov process. Set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. In the subsequent sections of this notebook you will improve this policy. \n",
    "\n",
    "> **Note:** As these are just starting values, the exact values of the transition probabilities are not actually all that important in terms of solving the DP problem. Also, notice that it does not matter how the taboo state transitions are encoded. The point of the DP algorithm is to learn the transition policy.  \n",
    "\n",
    "In the cell below, define a dictionary with the initial policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- +10 for achieving the goal. \n",
    "- -1 for attempting to leave the warehouse or hitting the barriers. In other words, we penalize the robot for hitting the edges of the grid or the barriers.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another.  \n",
    "\n",
    "In the code cell below encode a dictionary with your representation of this reward structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find it useful to create a list of taboo states, which you can encode in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "With your representation defined, you can now create and test a function for **policy evaluation**. You will need this function for your policy iteration code. \n",
    "\n",
    "You are welcome to start with the `compute_state_value` function from the DP notebook. However, keep in mind that you must modify this code to correctly treat the taboo states. Specifically, taboo states should have 0 value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the state values you have computed using a random walk for the robot. Answer the following questions:\n",
    "\n",
    "1. Are the values of the goal and taboo states zero? \n",
    "2. Do the values of the states increase closer to the goal? \n",
    "3. Do the goal and barrier states all have zero values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1:    \n",
    "ANS 2:    \n",
    "ANS 3:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Now that you have your representation and a functions to perform the MC policy evaluation you have everything you need to apply the policy improvement algorithm to create an optimal policy for the robot to reach the goal. \n",
    "\n",
    "If your policy evaluation functions works correctly, you should be able to use the `policy_iteration` function from the DP notebook with minor modifications. Make sure you print the state values as well as the policy you discovered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results. First look at the state values at convergence of the policy iteration algorithm and answer the following questions:\n",
    "1. Are non-taboo state values closest to the goal the largest? \n",
    "2. Are the non-taboo state values farthest from the goal the smallest? Keep in mind the robot must travel around the barrier. \n",
    "3. Are the non-taboo state values symmetric (e.g. same) with respect to distance from the goal? \n",
    "4. Do the taboo states have 0 values? \n",
    "5. How do the state values of the improved policy compare to the state values of the initial policy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1:     \n",
    "ANS 2:  \n",
    "ANS 3:   \n",
    "ANS 4:  \n",
    "ANS 5: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, examine the policy you have computed. Do the following:\n",
    "- Follow the optimal paths from the 4 corners of the grid to the goal. How does the symmetry and length of these paths make sense in terms of length and state values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imagine that the door for the warehouse is at position (state) 2. Insert an illustration showing the paths of the optimal plans below. You are welcome to start with the PowerPoint illustration in the course Github repository.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert your image here**    \n",
    " \n",
    "<img src=\"PlanOnGridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> Grid world optimal plans from state 2 to the goal shown goes here </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration \n",
    "\n",
    "Finally,  use the value iteration algorithm to compute an optimal policy for the robot reaching the goal. Keep in mind that you will need to maintain a state value of 0 for the taboo states. You may use the 'value_iteration' function from the DP notebook with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results from the value iteration algorithm to your results from the policy iteration algorithm and answer the following questions:   \n",
    "1. Are the state values identical between the two methods?    \n",
    "2. Ignoring the taboo states, are the plans computed by the two methods identical?    \n",
    "3. Ignoring the taboo states, are the final state values computed from both methods the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1:        \n",
    "ANS 2:      \n",
    "ANS 3:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
