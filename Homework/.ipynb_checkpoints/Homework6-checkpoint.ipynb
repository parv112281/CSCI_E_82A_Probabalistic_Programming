{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "\n",
    "In the Dynamic Programming (DP) lesson Jupyter notebook, we constructed a representation of a simple grid world. DP was used to find optimal plans for a robot to navigate from any starting location on the grid to the goal. This problem is an analog for more complex real-world robot navigation problems. \n",
    "\n",
    "In this homework you will use DP to solve a slightly more complex robotic navigation problem in a grid world. This grid world is a simple version of the problem a material transport robot might encounter in a warehouse. The situation is illustrated in the figure below.\n",
    "\n",
    "<img src=\"GridWorldFactory.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> Grid World for Factory Navigation Example </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is for the robot to deliver some material to position (state) 12, shown in blue. Since there is a goal state or **terminal state** this an **episodic task**. \n",
    "\n",
    "There are some barriers comprised of the states $\\{ 6, 7, 8 \\}$ and $\\{ 16, 17, 18 \\}$, shown with hash marks. In a real warehouse, these positions might be occupied by shelving or equipment. We do not want the robot to hit these barriers. Thus, we say that transitioning to these barrier states is **taboo**.\n",
    "\n",
    "As before, we do not want the robot to hit the edges of the grid world, which represent the outer walls of the warehouse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "As with many such problems, the starting place is creating the **representation**. In the cell below encode your representation for the possible action-state transitions. From each state there are 4 possible actions:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "There are a few special cases you need to consider:\n",
    "- Any action transitioning to a state off the grid or into a barrier should keep the state unchanged. \n",
    "- Once in the goal state there are no more state transitions. \n",
    "- Any transition within the barrier (taboo) states can keep the state unchanged. If you experiment, you will see that other encodings work as well since the value of a barrier states are always zero and there are no actions transitioning into these states. \n",
    "\n",
    "> **Hint:** It may help you create a pencil and paper sketch of the transitions, rewards, and probabilities or policy. This can help you to keep the bookkeeping correct. \n",
    "\n",
    "In the cell below define a dictionary where your code can look up the successor state given the current state and action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "\n",
    "neighbors = {\n",
    "              0:{'u':0, 'd':5, 'l':0, 'r':1},\n",
    "              1:{'u':1, 'd':1, 'l':0, 'r':2},\n",
    "              2:{'u':2, 'd':2, 'l':1, 'r':3},\n",
    "              3:{'u':3, 'd':3, 'l':2, 'r':4},\n",
    "              4:{'u':4, 'd':9, 'l':3, 'r':4},\n",
    "              5:{'u':0, 'd':10, 'l':5, 'r':5},\n",
    "              6:{'u':6, 'd':6, 'l':6, 'r':6},\n",
    "              7:{'u':7, 'd':7, 'l':7, 'r':7},\n",
    "              8:{'u':8, 'd':8, 'l':8, 'r':8},\n",
    "              9:{'u':4, 'd':14, 'l':9, 'r':9},\n",
    "              10:{'u':5, 'd':15, 'l':10, 'r':11},\n",
    "              11:{'u':11, 'd':11, 'l':10, 'r':12},\n",
    "              12:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "              13:{'u':13, 'd':13, 'l':12, 'r':14},\n",
    "              14:{'u':9, 'd':19, 'l':13, 'r':14},\n",
    "              15:{'u':10, 'd':20, 'l':15, 'r':15},\n",
    "              16:{'u':16, 'd':16, 'l':16, 'r':16},\n",
    "              17:{'u':17, 'd':17, 'l':17, 'r':17},\n",
    "              18:{'u':18, 'd':18, 'l':18, 'r':18},\n",
    "              19:{'u':14, 'd':24, 'l':19, 'r':19},\n",
    "              20:{'u':15, 'd':20, 'l':20, 'r':21},\n",
    "              21:{'u':21, 'd':21, 'l':20, 'r':22},\n",
    "              22:{'u':22, 'd':22, 'l':21, 'r':23},\n",
    "              23:{'u':23, 'd':23, 'l':22, 'r':24},\n",
    "              24:{'u':19, 'd':24, 'l':23, 'r':24},          \n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define the initial policy for the Markov process. Set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. In the subsequent sections of this notebook you will improve this policy. \n",
    "\n",
    "> **Note:** As these are just starting values, the exact values of the transition probabilities are not actually all that important in terms of solving the DP problem. Also, notice that it does not matter how the taboo state transitions are encoded. The point of the DP algorithm is to learn the transition policy.  \n",
    "\n",
    "In the cell below, define a dictionary with the initial policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy = {\n",
    "                    0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                    2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    12:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                    13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    16:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    17:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    18:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    19:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    20:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    21:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    22:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    23:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    24:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- +10 for achieving the goal. \n",
    "- -1 for attempting to leave the warehouse or hitting the barriers. In other words, we penalize the robot for hitting the edges of the grid or the barriers.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another.  \n",
    "\n",
    "In the code cell below encode a dictionary with your representation of this reward structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards =      {\n",
    "                    0:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "                    1:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1}, \n",
    "                    2:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    3:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    4:{'u':-1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "                    5:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    6:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    7:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    8:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    9:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    10:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "                    11:{'u':-1, 'd':-1, 'l':-0.1, 'r':10},\n",
    "                    12:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    13:{'u':-1, 'd':-1, 'l':10, 'r':-0.1},\n",
    "                    14:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "                    15:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    16:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    17:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    18:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    19:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    20:{'u':-0.1, 'd':-1, 'l':-1, 'r':-0.1},\n",
    "                    21:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    22:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    23:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    24:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-1}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find it useful to create a list of taboo states, which you can encode in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "taboo_states = [6,7,8,16,17,18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "With your representation defined, you can now create and test a function for **policy evaluation**. You will need this function for your policy iteration code. \n",
    "\n",
    "You are welcome to start with the `compute_state_value` function from the DP notebook. However, keep in mind that you must modify this code to correctly treat the taboo states. Specifically, taboo states should have 0 value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-38.28926029, -41.56189427, -42.65499935, -41.56857879,\n",
       "        -38.30169862],\n",
       "       [-32.84169421,   0.        ,   0.        ,   0.        ,\n",
       "        -32.8525447 ],\n",
       "       [-25.20972897,  -8.65258033,   0.        ,  -8.65482963,\n",
       "        -25.21851918],\n",
       "       [-32.84716098,   0.        ,   0.        ,   0.        ,\n",
       "        -32.85784594],\n",
       "       [-38.3016983 , -41.5751609 , -42.66847476, -41.58164302,\n",
       "        -38.31375999]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_state_value(neighbors, policy, reward, gamma = 1.0, theta = 0.01, display = False):\n",
    "    '''Function for policy evaluation  \n",
    "    '''\n",
    "    delta = theta\n",
    "    values = np.zeros(len(policy)) # Initialize the value array\n",
    "    while(delta >= theta):\n",
    "        v = np.copy(values) ## save the values for computing the difference later\n",
    "        for s in policy.keys():\n",
    "            temp_values = 0.0 ## Initial the sum of values for this state\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = neighbors[s][action]\n",
    "                temp_values = temp_values + policy[s][action] * (reward[s][action] + gamma * values[s_prime])\n",
    "            values[s] = temp_values\n",
    "            \n",
    "        ## Compute the difference metric to see if convergence has been reached.    \n",
    "        diffs = np.sum(np.abs(np.subtract(v, values)))\n",
    "        delta = min([delta, diffs])\n",
    "        if(display): \n",
    "            print('difference metric = ' + str(diffs))\n",
    "            print(values.reshape(5,5))\n",
    "    return values\n",
    "\n",
    "compute_state_value(neighbors, initial_policy, rewards, theta = 0.1, display = False).reshape(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the state values you have computed using a random walk for the robot. Answer the following questions:\n",
    "\n",
    "1. Are the values of the goal and taboo states zero? \n",
    "2. Do the values of the states increase closer to the goal? \n",
    "3. Do the goal and barrier states all have zero values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes\n",
    "ANS 2: Yes\n",
    "ANS 3: Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Now that you have your representation and a functions to perform the MC policy evaluation you have everything you need to apply the policy improvement algorithm to create an optimal policy for the robot to reach the goal. \n",
    "\n",
    "If your policy evaluation functions works correctly, you should be able to use the `policy_iteration` function from the DP notebook with minor modifications. Make sure you print the state values as well as the policy you discovered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 4: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 9: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 19: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 20: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 21: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 22: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 23: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 24: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 9.7  9.6  9.5  9.6  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(policy, neighbors, reward, gamma = 1.0, theta = 0.1, goal = 12, output = False):\n",
    "    ## As a first step compute the state values \n",
    "    state_values = compute_state_value(neighbors, policy, reward, gamma = gamma, theta = theta)\n",
    "    \n",
    "    ## Now, need to find the deterministic policy that \n",
    "    ## makes max value state transitions.\n",
    "    for s in policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        ## There are two cases. \n",
    "        ## First, the special case of a state adjacent to the goal\n",
    "        ## In this case need to ensure the only possible transition is to the goal.\n",
    "        ## Start by creating a list of the adjacent states.\n",
    "        possible_s_prime = [val for val in neighbors[s].values()]\n",
    "        ## Check if goal is adjacent, but state is not the goal.\n",
    "        if(goal in possible_s_prime and s != goal):\n",
    "            temp_values = []\n",
    "            for s_prime in policy[s].keys(): # Iterate over adjacent states\n",
    "                if(neighbors[s][s_prime] == goal):  ## account for the special case adjacent to goal\n",
    "                    temp_values.append(reward[s][s_prime])\n",
    "                else: temp_values.append(0.0) ## Other transisions have 0 value.\n",
    "        ## The other case is rather easy requires a lookup of the value of the \n",
    "        ## adjacent state and handled with a list comprehension.             \n",
    "        else: temp_values = [state_values[s_prime] for s_prime in neighbors[s].values()] \n",
    "                \n",
    "        ## Find the index for the state transistions with the largest values \n",
    "        ## May be more than one. \n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]  \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        \n",
    "        ##### ADD CODE TO FIND THE INDEX OF EACH MAXIMUM VALUE AND FIND  ##############\n",
    "        ##### THE PROBABIITY OF THE STATE TRANSITION FOR THE NUMBER OF   ##############\n",
    "        ##### MAXIMUM FOUND. THERE MAY BE MORE THAN ONE MAXIMUM          ##############\n",
    "        for i, key in enumerate(policy[s]): ## Update policy\n",
    "            if(i in max_index): policy[s][key] = prob_for_policy\n",
    "            else: policy[s][key] = 0.0       \n",
    "    return policy\n",
    "\n",
    "\n",
    "improved_policy = policy_iteration(initial_policy, neighbors, rewards, gamma = 1.0, output = True)\n",
    "new_state_values = compute_state_value(neighbors, improved_policy, rewards, gamma = 1.0, theta = 0.1)\n",
    "print(improved_policy)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(new_state_values.reshape(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results. First look at the state values at convergence of the policy iteration algorithm and answer the following questions:\n",
    "1. Are non-taboo state values closest to the goal the largest? \n",
    "2. Are the non-taboo state values farthest from the goal the smallest? Keep in mind the robot must travel around the barrier. \n",
    "3. Are the non-taboo state values symmetric (e.g. same) with respect to distance from the goal? \n",
    "4. Do the taboo states have 0 values? \n",
    "5. How do the state values of the improved policy compare to the state values of the initial policy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes  \n",
    "ANS 2: Yes\n",
    "ANS 3: Yes\n",
    "ANS 4: Yes\n",
    "ANS 5: The state values of the improved policy appear to hew more closely to the actual values that would be expected in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, examine the policy you have computed. Do the following:\n",
    "- Follow the optimal paths from the 4 corners of the grid to the goal. How does the symmetry and length of these paths make sense in terms of length and state values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: Since the corners are the exact same distance from the goal, the state values are symmetrically increasing in the direction of the optimal path from each corner to the goal and these paths are all the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imagine that the door for the warehouse is at position (state) 2. Insert an illustration showing the paths of the optimal plans below. You are welcome to start with the PowerPoint illustration in the course Github repository.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert your image here**    \n",
    " \n",
    "<img src=\"PlanOnGridWorld.JPG\" alt=\"Drawing\" style=\"width:400px; height:400px\"/>\n",
    "<center> Grid world optimal plans from state 2 to the goal shown goes here </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration \n",
    "\n",
    "Finally,  use the value iteration algorithm to compute an optimal policy for the robot reaching the goal. Keep in mind that you will need to maintain a state value of 0 for the taboo states. You may use the 'value_iteration' function from the DP notebook with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 3: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 4: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 9: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 19: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 20: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 21: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 22: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 23: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 24: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}}\n",
      "[[ 9.7  9.6  9.5  9.6  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(policy, neighbors, reward, goal, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(neighbors))\n",
    "    state_values = np.zeros(len(neighbors))\n",
    "    while(delta >= theta):\n",
    "        for s in neighbors.keys(): # iteratve over all states\n",
    "            temp_values = []\n",
    "            ## Find the values for all possible actions in the state.\n",
    "#            for action in rewards[s].keys():\n",
    "            for action in neighbors[s].keys():    \n",
    "                s_prime = neighbors[s][action]\n",
    "                temp_values.append((reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update the value for the state\n",
    "            #### ADD THE CODE TO FIND THE MAXIMUM OF THE temp_values #######\n",
    "            #### AND UPDATE THE STATE VALUE ARRAY                    #######\n",
    "            state_values[s] = max(temp_values)\n",
    "        ## Determine if convergence is achieved\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values)\n",
    "        if(output):\n",
    "            print('Difference = ' + str(diff))\n",
    "            print(state_values.reshape(4,4))\n",
    "    \n",
    "    ## Now we need to find the deterministic policy that \n",
    "    ## makes max value state transitions.\n",
    "    for s in policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        ## There are two cases. \n",
    "        ## First, the special case of a state adjacent to the goal\n",
    "        ## In this case need to ensure the only possible transition is to the goal.\n",
    "        ## Start by creating a list of the adjacent states.\n",
    "        possible_s_prime = [state for state in neighbors[s].values()]\n",
    "        ## Check if goal is adjacent, but state is not the goal.\n",
    "        if(goal in possible_s_prime and s != goal):\n",
    "            temp_values = []\n",
    "            for a_prime in neighbors[s].keys(): # Iterate over adjacent states\n",
    "                if(neighbors[s][a_prime] == goal):  ## account for the special case adjacent to goal\n",
    "                    temp_values.append(reward[s][a_prime])\n",
    "                else: temp_values.append(0.0) ## Other transisions have 0 value.\n",
    "        ## The other case is rather easy requires a lookup of the value of the \n",
    "        ## adjacent state and handled with a list comprehension.             \n",
    "        else: temp_values = [state_values[s_prime] for s_prime in neighbors[s].values()]\n",
    "                \n",
    "        ## Find the index for the state transistions with the largest values \n",
    "        ## May be more than one. \n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]  \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        for i, key in enumerate(policy[s]): ## Update policy\n",
    "            if(i in max_index): policy[s][key] = prob_for_policy\n",
    "            else: policy[s][key] = 0.0       \n",
    "    return policy\n",
    "\n",
    "value_iteration_policy = value_iteration(initial_policy, neighbors, rewards, goal = 12,  output = False)\n",
    "value_iteration_sv = compute_state_value(neighbors, value_iteration_policy, rewards, theta = 0.1, display = False).reshape(5,5)\n",
    "print(value_iteration_policy)\n",
    "print(value_iteration_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results from the value iteration algorithm to your results from the policy iteration algorithm and answer the following questions:   \n",
    "1. Are the state values identical between the two methods?    \n",
    "2. Ignoring the taboo states, are the plans computed by the two methods identical?    \n",
    "3. Ignoring the taboo states, are the final state values computed from both methods the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes       \n",
    "ANS 2: Yes  \n",
    "ANS 3: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
