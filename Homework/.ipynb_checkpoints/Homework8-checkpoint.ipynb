{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "\n",
    "In the a previous homework assignments, you used two different dynamic programming algorithm to solve a robot navigation problem by finding optimal paths to a goal in a simplified warehouse environment. Now you will use classical reinforcement learning algorithms to find optimal paths in the same environment.\n",
    "\n",
    ">**Note:** This assignment involves modifying code from several lesson notebooks. Please ask any questions about code on the course Piazza forum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to load the packages you will need for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required packages\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the warehouse environment is illustrated in the figure below.\n",
    "\n",
    "<img src=\"GridWorldFactory.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for Factory Navigation Example** </center>\n",
    "\n",
    "The goal is for the robot to deliver some material to position (state) 12, shown in blue. Since there is a goal state or **terminal state** this an **episodic task**. \n",
    "\n",
    "There are some barriers comprised of the states $\\{ 6, 7, 8 \\}$ and $\\{ 16, 17, 18 \\}$, shown with hash marks. In a real warehouse, these positions might be occupied by shelving or equipment. We do not want the robot to hit these barriers. Thus, we say that transitioning to these barrier states is **taboo**.\n",
    "\n",
    "As before, we do not want the robot to hit the edges of the grid world, which represent the outer walls of the warehouse. \n",
    "\n",
    "## Representation\n",
    "\n",
    "You are, no doubt, familiar with the representation for this problem by now.    \n",
    "\n",
    "As with many such problems, the starting place is creating the **representation**. In the cell below encode your representation for the possible action-state transitions. From each state there are 4 possible actions:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "There are a few special cases you need to consider:\n",
    "- Any action transitioning state off the grid or into a barrier (taboo) state should keep the state unchanged. \n",
    "- Any action in the goal state keeps the state unchanged. \n",
    "- Any transition within the taboo (barrier) states can keep the state unchanged. If you experiment, you will see that other encodings work as well since the value of a barrier states are always zero and there are no actions transitioning into these states. \n",
    "\n",
    "In the cell below code the **dictionary  of dictionaries** which defines the possible state transitions to the neighboring states.\n",
    "\n",
    "> **Hint:** It may help you create a pencil and paper sketch of the transitions, rewards, and probabilities or policy. This can help you to keep the bookkeeping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {\n",
    "              0:{'u':0, 'd':5, 'l':0, 'r':1},\n",
    "              1:{'u':1, 'd':1, 'l':0, 'r':2},\n",
    "              2:{'u':2, 'd':2, 'l':1, 'r':3},\n",
    "              3:{'u':3, 'd':3, 'l':2, 'r':4},\n",
    "              4:{'u':4, 'd':9, 'l':3, 'r':4},\n",
    "              5:{'u':0, 'd':10, 'l':5, 'r':5},\n",
    "              6:{'u':6, 'd':6, 'l':6, 'r':6},\n",
    "              7:{'u':7, 'd':7, 'l':7, 'r':7},\n",
    "              8:{'u':8, 'd':8, 'l':8, 'r':8},\n",
    "              9:{'u':4, 'd':14, 'l':9, 'r':9},\n",
    "              10:{'u':5, 'd':15, 'l':10, 'r':11},\n",
    "              11:{'u':11, 'd':11, 'l':10, 'r':12},\n",
    "              12:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "              13:{'u':13, 'd':13, 'l':12, 'r':14},\n",
    "              14:{'u':9, 'd':19, 'l':13, 'r':14},\n",
    "              15:{'u':10, 'd':20, 'l':15, 'r':15},\n",
    "              16:{'u':16, 'd':16, 'l':16, 'r':16},\n",
    "              17:{'u':17, 'd':17, 'l':17, 'r':17},\n",
    "              18:{'u':18, 'd':18, 'l':18, 'r':18},\n",
    "              19:{'u':14, 'd':24, 'l':19, 'r':19},\n",
    "              20:{'u':15, 'd':20, 'l':20, 'r':21},\n",
    "              21:{'u':21, 'd':21, 'l':20, 'r':22},\n",
    "              22:{'u':22, 'd':22, 'l':21, 'r':23},\n",
    "              23:{'u':23, 'd':23, 'l':22, 'r':24},\n",
    "              24:{'u':19, 'd':24, 'l':23, 'r':24},          \n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- +10 for entering position 12. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the MC RL agent**. The agent must **learn** the rewards by sampling the environment. \n",
    "\n",
    "In the code cell below encode the **dictionary  of dictionaries** for your representation of this reward structure you will use in your simulated environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards =      {\n",
    "                    0:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "                    1:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1}, \n",
    "                    2:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    3:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    4:{'u':-1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "                    5:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    6:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    7:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    8:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    9:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    10:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "                    11:{'u':-1, 'd':-1, 'l':-0.1, 'r':10},\n",
    "                    12:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    13:{'u':-1, 'd':-1, 'l':10, 'r':-0.1},\n",
    "                    14:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "                    15:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    16:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    17:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    18:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "                    19:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "                    20:{'u':-0.1, 'd':-1, 'l':-1, 'r':-0.1},\n",
    "                    21:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    22:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    23:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "                    24:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-1}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Simulator\n",
    "\n",
    "You must create a simulator for the grid world environment in the code below. Your RL agents will have no information on the environment, and will only interact through the simulator. The simulator must have at least the following characteristics:\n",
    "\n",
    "1. Use the representations of the environment defined above.\n",
    "2. Have arguments of state, action, terminal state and environment representation. \n",
    "3. Returns the state-prime, reward and if the state is the terminal state. \n",
    "\n",
    "You will also need a utility function to determine if a state is either terminal or taboo. This function will be used for generating random starts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 12):\n",
    "    \"\"\"\n",
    "    Function simulates the environment\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward = rewards[s][action]\n",
    "    return (s_prime, reward, is_terminal(s_prime, terminal))\n",
    "\n",
    "def is_taboo_or_terminal(state, terminal = 12):\n",
    "    taboo_states = [6,7,8,16,17,18]\n",
    "    return state in taboo_states or is_terminal(state, terminal)\n",
    "\n",
    "def is_terminal(state, terminal = 12):\n",
    "    return state == terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Episode Function\n",
    "\n",
    "To do random starts, you will need a function to generate them. Your function must ensure that the start state is not the terminal state or a taboo state. In the cell below create and test the required code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_state(n_states):\n",
    "    '''Function to find a random starting value for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_taboo_or_terminal(state)):\n",
    "         state = nr.choice(range(n_states))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Uniform Policy \n",
    "\n",
    "You need to define the initial transition probabilities for the Markov process. In the cell below create a **dictionary  of dictionaries** to set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. \n",
    "\n",
    "> **Note:** As these are just starting values, the exact values of the transition probabilities are not actually all that important in terms of solving the RL problem. Also, notice that it does not matter how the taboo state transitions are encoded. The point of the RL algorithm is to learn the transition policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy = {\n",
    "                    0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                    2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    12:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                    13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    16:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    17:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    18:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    19:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    20:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    21:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    22:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    23:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                    24:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find it useful to create a list of taboo states, which you can encode in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo State Value Policy Evaluation\n",
    "\n",
    "With your representations and the environment simulator defined, you can now create and test functions to perform MC RL **policy evaluation**. \n",
    "\n",
    "In the cell below create and test a function for taking an action in the environment. You may start with the `take_action` function from the lesson notebook. This function should have the following characteristics:\n",
    "\n",
    "1. Have arguments of state and policy.\n",
    "2. Compute the action, given the probabilities of the policy.\n",
    "2. Return the action, the successor state, the reward, and a flag indicating if the successor state is terminal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state, policy, actions = {1:'u', 2:'d', 3:'l', 4:'r'}):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    action = actions[nr.choice(range(len(actions)), p = list(policy[state].values())) + 1]\n",
    "    s_prime, reward, is_terminal = simulate_environment(state, action)\n",
    "    return (action, s_prime, reward, is_terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Episode Generation\n",
    "\n",
    "As a first step you will need a function to generate a single first visit Monte Carlo episode for state value estimation, given the policy in the warehouse environment. You may start with the `MC_generate_episode` function from the MC RL notebook as a starting point.    \n",
    "\n",
    "Make sure that taboo states are not visited on the random walk and that each episode ends at the terminal state. These key properties are \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_episode(policy, G, n_visits, n_states): \n",
    "    '''Function creates the Monte Carlo samples of one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    states_visited = []\n",
    "    states = list(policy.keys())\n",
    "        \n",
    "    ## Find the starting state\n",
    "    current_state = get_start_state(n_states)\n",
    "    terminal = False\n",
    "        \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "            \n",
    "        ## Add the reward to the states visited if this is a first visit  \n",
    "        if(current_state not in states_visited):\n",
    "            ## Mark that the current state has been visited \n",
    "            states_visited.append(current_state) \n",
    "            ## Add the reward to states visited \n",
    "            for state in states_visited:\n",
    "                n_visits[state] = n_visits[state] + 1.0\n",
    "                G[state] = G[state] + (reward - G[state])/n_visits[state]\n",
    "        \n",
    "        ## Update the current state for next transition\n",
    "        current_state = s_prime \n",
    "    return (G, n_visits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the algorithm used in the `MC_generate_episode` considered first visit Monte Carlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: This algorithm is called first visit Monte Carlo because it tracks rewards for a state dusring an episore starting at the first visit and each subsequent visit to that state during an episode does not create a new entry for tracking rewards for that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have all the components in place to use the Monte Carlo algorithm to compute state values. You may use the `MC_state_values` function from the lesson notebook as a starting point. The main work done in this function is to iterate over the required number of episodes to compute the MC state value estimates. \n",
    "\n",
    "Execute your function for 5,000 episodes, to evaluate the initial uniform policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17542112 -0.19345171 -0.19696165 -0.18622019 -0.17951876]\n",
      " [-0.14142168  0.          0.          0.         -0.14902569]\n",
      " [-0.03996262  0.45275081  0.          0.47150649 -0.04375415]\n",
      " [-0.14577892  0.          0.          0.         -0.13977576]\n",
      " [-0.17498279 -0.18593679 -0.18702388 -0.18779633 -0.16499658]]\n"
     ]
    }
   ],
   "source": [
    "def MC_state_values(policy, n_episodes):\n",
    "    '''Function that evaluates the state value of \n",
    "    a policy using the Monte Carlo method.'''\n",
    "    ## Create list of states \n",
    "    states = list(initial_policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(n_episodes):\n",
    "        G, n_visits = MC_episode(policy, G, n_visits, n_states)\n",
    "    return(G) \n",
    "\n",
    "nr.seed(335)\n",
    "state_values = MC_state_values(initial_policy, n_episodes = 5000)\n",
    "print(state_values.reshape((5,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the state values of the terminal state and taboo state must be zero. If they are not, there must be something wrong with your code.  \n",
    "\n",
    "Examine your results. Are do the state values decrease with distance from the terminal state, and is this consistent with sampling the gain with the Monte Carlo algorithm? With 5,000 episodes sampled can you see evidence of the high variance inherent in MC methods? To answer the this second question it might help you to re-compute the state values using fewer or more episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes, the state values decrease with distance away from the goal state. This is consistent with MC algorithm because states closer to the goal state are more likely to quickly converge to goal state and thus have higher overall value.\n",
    "\n",
    "ANS 2: The higher variance of MC methods is readily apparent when examining the values of mirror states against each other. For example, state 0 and state 4 should have similar values since they are the same distance away from the goal state, but they differ from each other by approximately 0.04. This holds true for the other mirror states as well, especially states 19 and 24 which differ by 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Improvement\n",
    "\n",
    "Now you are ready to perform Monte Carlo policy improvement. As a first step you will need a function to compute the action values (Q values) for one episode. You may start with the `MC_action_value_episode` function from the lesson notebook. This function takes the following arguments, the current policy dictionary, the Q numpy array, the number of visits numpy array, the starting state, the number of states, and the number of actions. The function returns the updated Q numpy array and the number of visits numpy array. In the cell below create and test this function.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def MC_action_value_episode(policy, Q, n_visits, inital_state, n_states, n_actions, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Function creates the Monte Carlo samples of action values for one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    state_actions_visited = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    current_state = get_start_state(n_states)\n",
    "    terminal = False  \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "\n",
    "        action_idx = action_index[action]         \n",
    "        \n",
    "        ## Check if this state-action has been visited.\n",
    "        if(state_actions_visited[current_state, action_idx] != 1.0):\n",
    "            ## Mark that the current state-action has been visited \n",
    "            state_actions_visited[current_state, action_idx] = 1.0  \n",
    "            ## This is first vist MS, so must loop over all state-action pairs and \n",
    "            ## add the reward and increment the count for the ones visited.\n",
    "            for s,a in list(itertools.product(range(n_states), range(n_actions))):\n",
    "                ## Add reward to if these has been a visit to the state\n",
    "                if(state_actions_visited[s,a] == 1.0):\n",
    "                    n_visits[s,a] = n_visits[s,a] + 1.0\n",
    "                    Q[s,a] = Q[s,a] + (reward - Q[s,a])/n_visits[s,a]    \n",
    "        ## Update the current state for next transition\n",
    "        current_state = s_prime\n",
    "    return (Q, n_visits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in the cell below you need to create and test a function to compute the action value estimates over a number of episodes. You may start with the 'MC_action_values' function from the TD lesson notebook. The function has arguments of the initial policy, the action value matrix, the number of episodes to sample and the inital or starting state. The function returns the action value matrix. Execute your function for 10000 episodes, saving the results in a numpy array, and print the array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up      down       left      right\n",
      "0  -0.057761  0.002561  -0.054009  -0.073226\n",
      "1  -0.078498 -0.076478  -0.048838  -0.083515\n",
      "2  -0.084256 -0.084184  -0.077502  -0.073923\n",
      "3  -0.075388 -0.074754  -0.083073  -0.045719\n",
      "4  -0.040974  0.004857  -0.070724  -0.046262\n",
      "5  -0.044885  0.095654  -0.003508   0.001851\n",
      "6   0.000000  0.000000   0.000000   0.000000\n",
      "7   0.000000  0.000000   0.000000   0.000000\n",
      "8   0.000000  0.000000   0.000000   0.000000\n",
      "9  -0.045096  0.105057   0.003052  -0.002860\n",
      "10  0.005476  0.004471   0.086113   0.541406\n",
      "11  0.572021  0.562418   0.131974  10.000000\n",
      "12  0.000000  0.000000   0.000000   0.000000\n",
      "13  0.585418  0.588796  10.000000   0.144162\n",
      "14 -0.002626  0.005086   0.557293   0.100499\n",
      "15  0.104960 -0.043289   0.002411   0.008493\n",
      "16  0.000000  0.000000   0.000000   0.000000\n",
      "17  0.000000  0.000000   0.000000   0.000000\n",
      "18  0.000000  0.000000   0.000000   0.000000\n",
      "19  0.100085 -0.041028   0.001281  -0.000747\n",
      "20  0.006287 -0.050941  -0.042070  -0.068428\n",
      "21 -0.074337 -0.073347  -0.044536  -0.077835\n",
      "22 -0.081067 -0.083267  -0.071356  -0.070982\n",
      "23 -0.070908 -0.069339  -0.076725  -0.042990\n",
      "24  0.005317 -0.046521  -0.063919  -0.043238\n"
     ]
    }
   ],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def MC_action_values(policy, Q, n_episodes, inital_state):\n",
    "    '''Function evaluates the action-values given a policy for the specified number of episodes and \n",
    "    initial state'''\n",
    "    n_states = len(policy)\n",
    "    n_actions = len(policy[0])\n",
    "    ## Array to count visits to action-value pairs\n",
    "    n_visits = np.zeros((n_states, n_actions))\n",
    "    ## Dictionary to hold neighbor states\n",
    "    neighbors = {}\n",
    "    \n",
    "    ## Loop over number of episodes\n",
    "    for _ in range(n_episodes):\n",
    "        ## One episode of MC\n",
    "        Q, n_visits = MC_action_value_episode(policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "    return(Q)\n",
    "\n",
    "## Basic test of the function\n",
    "n_episodes = 10000\n",
    "initial_state = 2\n",
    "n_states = 25\n",
    "n_actions = 4\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "Q = MC_action_values(initial_policy, Q, n_episodes, initial_state)\n",
    "print_Q(Q)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the taboo states and the terminal state all have zero action values for every action. If not, there is a bug in your code. \n",
    "\n",
    "Find the largest action values in this array. Are these values consistent with the reward structure of this problems and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: The larges action values belong to actions within states that directly transistion to the goal state. This is consistent because the goal and therefore the reward is achieved by transitioning to goal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the action value array computed, in the cell below you can create and test code to find an improved policy. You may use the `update_policy` function from the TD lesson notebook. The function has arguments of an initial policy, an action value array, the $\\epsilon$-greedy parameter, and returns an improved policy. \n",
    "\n",
    "Before executing your `update_policy` function will need to create a deep copy of your initial dictionary. Do this with the [copy.deepcopy function](https://docs.python.org/3/library/copy.html).  \n",
    "\n",
    "Execute your code with $\\epsilon = 0.01$, and print the policy computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 1: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 2: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 3: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 4: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 5: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 9: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 10: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 11: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 13: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 14: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 15: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 19: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 20: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 21: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 22: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 23: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 24: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_policy(policy, Q, epsilon, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(action_index[key] in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)             \n",
    "\n",
    "policy = deepcopy(initial_policy)\n",
    "policy = update_policy(policy, Q, 0.01)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your impoved policy. Are the most probable actions consitent with an optimal policy and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: Yes, the most probable actions are consistent with the optimal policy since they all directly lead to the goal state via the shortest possible path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute and print the Monte Carlo state values for the improved policy in the cell below. Use 10,000 episodes to compute these state value estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up      down       left      right\n",
      "0   1.740000  2.347354   1.712121   1.618103\n",
      "1   1.740000  1.433333   1.849859   1.293103\n",
      "2   1.094595  1.214286   1.583333   1.524119\n",
      "3   1.405455  1.408197   1.352727   1.849771\n",
      "4   1.803448  2.336647   1.613415   1.666129\n",
      "5   2.035043  3.184046   2.200000   2.116667\n",
      "6   0.000000  0.000000   0.000000   0.000000\n",
      "7   0.000000  0.000000   0.000000   0.000000\n",
      "8   0.000000  0.000000   0.000000   0.000000\n",
      "9   2.249550  3.160560   2.145763   2.121951\n",
      "10  2.666250  2.516162   2.885185   4.857670\n",
      "11  4.500000  4.500000   4.514167  10.000000\n",
      "12  0.000000  0.000000   0.000000   0.000000\n",
      "13  4.415385  4.500000  10.000000   4.657480\n",
      "14  2.769874  2.551707   4.855099   2.940397\n",
      "15  3.171202  2.007368   2.168493   2.043421\n",
      "16  0.000000  0.000000   0.000000   0.000000\n",
      "17  0.000000  0.000000   0.000000   0.000000\n",
      "18  0.000000  0.000000   0.000000   0.000000\n",
      "19  3.170880  2.259813   2.232394   2.169524\n",
      "20  2.349060  1.692105   1.652381   1.685556\n",
      "21  1.389091  1.251064   1.860524   1.277273\n",
      "22  1.179688  1.170000   1.583333   1.526747\n",
      "23  1.414583  1.389524   1.381818   1.850593\n",
      "24  2.346871  1.720833   1.754369   1.703947\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((n_states, n_actions))\n",
    "Q = MC_action_values(policy, Q, n_episodes, initial_state)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these state values to the MC state values you computed using the initial random policy. Is there a clear improvement across all states?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: Yes, these values are closer to the actual values expected for the action values and the actions that most directly lead to the goal state have the highest values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(0) Policy Evaluation\n",
    "\n",
    "With the analysis using Monte Carlo RL algorithms completed, you will perform the same analysis using the TD(0)/SARSA(0) methods. You will then be able to compare the results using these two methods.  \n",
    "\n",
    "As a first step you will need a function to compute state values using the TD(0) algorithm. You are welcome to start with the `td_0_state_values` function from the TD learning notebook. The arguments to this function are policy the number of episodes, alpha (the learning rate) and gamma (discount factor). This function uses the `take_action` function you defined previously. Keep in mind that you may need to modify this code to correctly treat the taboo states. Specifically, taboo states should not be visited. \n",
    "\n",
    "Execute your code for the initial policy to test it using 20,000 episodes, a learning rate of 0.01 and a discount rate of 1.0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.55875201, -6.16665829, -6.43871617, -6.16210545, -5.62079724],\n",
       "       [-4.48276975,  0.        ,  0.        ,  0.        , -4.21467527],\n",
       "       [-1.93174356,  2.83131451,  0.        ,  2.65194047, -2.14045332],\n",
       "       [-4.28343998,  0.        ,  0.        ,  0.        , -4.49126934],\n",
       "       [-5.92117886, -6.80216857, -7.04257222, -6.82536281, -5.96633655]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def td_0_state_values(policy, n_samps, alpha = 0.01, gamma = 1.0):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy evalutation\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Find the starting state\n",
    "    n_states = len(policy)\n",
    "    current_state = get_start_state(n_states)\n",
    "    terminal = False\n",
    "    \n",
    "    ## Array for state values\n",
    "    v = np.zeros((n_states,1))\n",
    "    \n",
    "    for _ in range(n_samps):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        ## Compute the TD error\n",
    "        delta = reward + gamma*v[s_prime] - v[current_state]\n",
    "        ## Update the state value\n",
    "        v[current_state] = v[current_state] + alpha*delta\n",
    "        current_state = s_prime\n",
    "        if(terminal): ## start new episode when terminal\n",
    "            current_state = get_start_state(n_states)\n",
    "    return(v)\n",
    "\n",
    "td_0_state_values(initial_policy, 20000).reshape((5,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the state values of the terminal state and taboo state must be zero. If they are not, there must be something wrong with your code.  \n",
    "\n",
    "Examine your results. Do these state values seem consistent with what you would expect for this problem, and why? Compare these results to the unbiased results of the MC state value estimates. Can you see evidence of the bias of the TD(0) algorithm, and if so, what is it? To explore the answer to the second question you can try different values of the learning rate, alpha and see how this affects the bias. If you use small values of alpha, you will need to increase the number of episodes sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes these state values meet expectations since they decrease away from the goal state.\n",
    "\n",
    "ANS 2: Yes, evidence of the bias is readily apparent since the values determined by this algorithm tends to take more extreme values for the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SARSA(0) Policy Improvement\n",
    "\n",
    "Now you will perform policy improvement using the SARSA(0) algorithm.  You are welcome to start with the `new_episode` and `SARSA_0` functions from the TD/Q-learning notebooks.    \n",
    "\n",
    "The `new_episode` function starts an episode to sample action values. This function has arguments of number of states and the policy. The function calls the `start_episode` and `take_action` functions you created previously. The function returns the starting state of the episode, the action, the successor state, the reward, and a flag indicating if the successor state is terminal. \n",
    "\n",
    "The `SARSA_0` function estimates action value (Q) over a specified number of episodes. The function has arguments of policy, number of episodes sampled, the learning rate (alpha) and the discount rate (gamma). The function uses the `new episode` and `take_action` functions. The state and successor state information is updated at each time step. An action value array is returned.  \n",
    "\n",
    "Execute your code for 20,000 episodes, and with $\\alpha = 0.05$, and $\\gamma = 1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up      down      left     right\n",
      "0  -8.623055 -6.296906 -8.678198 -8.289780\n",
      "1  -9.250954 -9.288359 -7.618150 -8.240571\n",
      "2  -9.192581 -9.084998 -8.155129 -8.122144\n",
      "3  -8.744273 -8.788383 -8.155055 -7.461992\n",
      "4  -8.953603 -6.704727 -7.797489 -8.396222\n",
      "5  -7.391951 -4.984876 -6.774654 -7.226097\n",
      "6   0.000000  0.000000  0.000000  0.000000\n",
      "7   0.000000  0.000000  0.000000  0.000000\n",
      "8   0.000000  0.000000  0.000000  0.000000\n",
      "9  -7.581507 -4.873750 -7.285858 -7.589021\n",
      "10 -6.172268 -5.713789 -5.110744 -2.159660\n",
      "11 -3.181519 -3.201073 -3.993894 -0.612383\n",
      "12  0.000000  0.000000  0.000000  0.000000\n",
      "13 -3.026403 -3.612226 -0.593345 -4.374524\n",
      "14 -6.194250 -6.283926 -2.729904 -6.334686\n",
      "15 -4.646245 -6.672366 -6.873907 -6.777552\n",
      "16  0.000000  0.000000  0.000000  0.000000\n",
      "17  0.000000  0.000000  0.000000  0.000000\n",
      "18  0.000000  0.000000  0.000000  0.000000\n",
      "19 -5.018837 -7.492853 -7.741213 -7.459386\n",
      "20 -5.930288 -7.472742 -7.468818 -7.148143\n",
      "21 -8.274179 -8.237679 -6.753658 -7.669712\n",
      "22 -8.820659 -8.775556 -7.278648 -7.802877\n",
      "23 -8.671225 -8.558177 -7.881766 -7.608352\n",
      "24 -6.839282 -8.360615 -7.866746 -8.449242\n"
     ]
    }
   ],
   "source": [
    "def new_episode(n_states, policy):\n",
    "    '''This function provides a start for a TD\n",
    "    episode making sure the first transition is not \n",
    "    the termnal state'''\n",
    "    current_state = get_start_state(n_states)\n",
    "    ## Find fist action and reward\n",
    "    action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "    return(current_state, action, s_prime, reward, terminal)\n",
    "\n",
    "def SARSA_0(policy, n_samps, alpha = 0.05, gamma = 1.0, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy evalutation\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Find the starting state\n",
    "    n_states = len(policy)\n",
    "    current_state, action, s_prime, reward, terminal = new_episode(n_states, policy)\n",
    "    action_idx = action_index[action]\n",
    "    \n",
    "    ## Array for state values\n",
    "    q = np.zeros((n_states, len(policy[0])))\n",
    "    \n",
    "    for i in range(n_samps):\n",
    "        ## Find the next action and reward\n",
    "        action_prime, s_prime_prime, reward_prime, terminal_prime = take_action(s_prime, policy)\n",
    "        action_idx_prime = action_index[action_prime]\n",
    "        ## Compute the TD error\n",
    "        delta = reward + gamma*q[s_prime, action_idx_prime] - q[current_state, action_idx]\n",
    "        ## Update the action values\n",
    "        q[current_state, action_idx] = q[current_state, action_idx] + alpha*delta\n",
    "        ## Update the state, action and reward for the next time step\n",
    "        current_state = s_prime\n",
    "        s_prime = s_prime_prime\n",
    "        action = action_prime\n",
    "        reward = reward_prime\n",
    "        terminal = terminal_prime\n",
    "        action_idx = action_idx_prime\n",
    "\n",
    "        ## Check if end of episode\n",
    "        while(terminal): \n",
    "            ## start new episode\n",
    "            current_state, action, s_prime, reward, terminal = new_episode(n_states, policy)        \n",
    "    return(q)\n",
    "\n",
    "Q = SARSA_0(initial_policy, 20000)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the action values you have computed. Ensure that the action values are 0 for the goal and taboo states. Also check that the actions with the largest values for each state make sense in terms of reaching the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the action values computed, you will now find an improved policy. Make sure you create a copy of the initial random policy with `copy.deepcopy`. Execute the `update_policy` function using the copy of the initial random policy, the action value array you computed, and with $\\epsilon = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 1: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 2: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 3: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 4: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 5: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 9: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 10: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 11: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 13: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 14: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 15: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 19: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 20: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 21: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 22: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 23: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 24: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = deepcopy(initial_policy)\n",
    "policy = update_policy(policy, Q, 0.01)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the policy you have computed. Make sure that the most probable actions lead to an optimal path to the terminal state from any other state, that is not taboo. \n",
    "\n",
    "Now, you will evaluate your policy using both the MC and TD(0) algorithms. In the cells below execute the required functions and display the results. For the `MC_state_values` function use 10,000 episodes.  For the `td_0_state_values` function use 20,000 episodes, a learning rate of 0.01 and a discount rate of 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.33189548, 1.84360759, 1.48965517, 1.81314835, 2.28125944,\n",
       "       3.15983561, 0.        , 0.        , 0.        , 3.09375467,\n",
       "       4.74978498, 9.65609756, 0.        , 9.58402414, 4.70664864,\n",
       "       3.13906671, 0.        , 0.        , 0.        , 3.15342357,\n",
       "       2.31243251, 1.81983604, 1.5118122 , 1.85701262, 2.33771571])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_state_values(policy, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.52728947, 8.10929777, 7.30802854, 9.1867243 , 9.59489678],\n",
       "       [9.74136572, 0.        , 0.        , 0.        , 9.74000143],\n",
       "       [9.86783418, 9.99007503, 0.        , 9.96828628, 9.84554172],\n",
       "       [9.75213663, 0.        , 0.        , 0.        , 9.73222868],\n",
       "       [9.57359678, 9.12988153, 6.86568717, 7.4690226 , 9.38937834]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_0_state_values(policy, 20000, 0.01, 1.0).reshape((5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the state values computed by the two algorithms. Do both methods compute state-values consistent with the problem, and why? Why do you think these estimates of state value are different? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes, both algorithms produce state values that decrease the futher the state is from the goal.\n",
    "\n",
    "ANS 2: The TD(0) policy produces values that are higher because of its bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Double Q-Learning\n",
    "\n",
    "Finally, you will apply Double Q-learning(0) to the warehouse navigation problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Environment Simulator\n",
    "\n",
    "Since Q-learning is different from MC or TD/SARSA learning, you will need to create a new set of environment simulator functions. Keep in mind that the simulator is not part of the agent and the agent can only interact with the environment through the simulations. \n",
    "\n",
    "You may use the Q-learning simulator code from the Q-learning notebook as a starting point to create the following functions. It will help to change the names of these functions, by say, prefixing Q_ to prevent name space conflicts with previously defined functions. \n",
    "\n",
    "\n",
    "**Q_action_lookup** and **Q_index_lookup** functions to translate between an index and the corresponding action or the action and the corresponding index.  \n",
    "\n",
    "**Q_next_state** function returns the next state given arguments of state and action. \n",
    "\n",
    "**Q_simulate_environment** function is similar to the `simulate_environment` function you have been working with. The arguments are state and action. The function then returns the successor or state, the an array of rewards from the successor state, and a flag to indicate if the successor state is terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_action_lookup(index):\n",
    "    \"\"\"Helper function returns action given an index\"\"\"\n",
    "    action_dic = {0:'u', 1:'d', 2:'l', 3:'r'}\n",
    "    return action_dic[index]\n",
    "\n",
    "def Q_index_lookup(action):\n",
    "    \"\"\"Helper function returns index given action\"\"\"\n",
    "    index_dic = {'u':0, 'd':1, 'l':2, 'r':3}\n",
    "    return index_dic[action]\n",
    "\n",
    "\n",
    "def Q_next_state(state, action_index, neighbors = neighbors, action_lookup = Q_action_lookup):\n",
    "    return(neighbors[state][action_lookup[action_index]])\n",
    "\n",
    "def Q_simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 12):\n",
    "    \"\"\"\n",
    "    Function simulates the environment for Q-learning.\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward_prime = np.array([rewards[s_prime][a] for a in rewards[0].keys()])\n",
    "    return (s_prime, reward_prime, is_terminal(s_prime, terminal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Episode Start\n",
    "\n",
    "In the cell below, create and test a function to find a random start for each episode. You can define the `Q_start_episode` function starting with the function in the MC RL lesson notebook. The function takes arguments of number of states and number of actions. The function uses the `is_terminal` and `Q_simulate_environment` functions you defined earlier. The function returns the state, initial action index and the initial reward. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 1, -0.1),\n",
       " (0, 2, -1.0),\n",
       " (13, 2, 0),\n",
       " (14, 1, -0.1),\n",
       " (5, 0, -1.0),\n",
       " (11, 2, -1.0),\n",
       " (2, 1, -1.0),\n",
       " (10, 0, -0.1),\n",
       " (3, 3, -1.0),\n",
       " (1, 3, -0.1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Q_start_episode(n_states, n_actions):\n",
    "    '''Function to find a random starting values for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_taboo_or_terminal(state)):  ## Make sure not starting at the terminal state\n",
    "         state = nr.choice(range(n_states))\n",
    "    ## Now find a random starting action index\n",
    "    a_index = nr.choice(range(4), size = 1)[0]\n",
    "    s_prime, reward, terminal = Q_simulate_environment(state, Q_action_lookup(a_index))   \n",
    "    return state, a_index, reward[a_index] \n",
    "\n",
    "## test the function to make sure never starting in terminal state\n",
    "[Q_start_episode(15,4) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning    \n",
    "\n",
    "It is now time to pull together the functions to implement the double Q-learning algorithm. There are two functions, which you can use the functions from the MC lesson notebook as a starting point.   \n",
    "\n",
    "**update_double_Q** is a function that computes the error, using the double Q update, and updates the Q value. The function takes the arguments of the two Q arrays, the current state, the action index, the current reward, the learning rate (alpha) and the discount rate (gamma). The function uses the `Q_simulate_environment` function. The function returns the updated Q array, the successor state, the successor reward, a flag indicating if the successor state is terminal and the index of the successor action.   \n",
    "\n",
    "**double_Q_learning_0** is a function that randomly updates one or the other Q arrays using the Q array as an argument over the specified number of episodes. The function has arguments of policy, number of episodes, the learning rate (alpha), and the discount rate (gamma). The function makes calls to the `update_double_Q` function. An updated Q array is returned.   \n",
    "\n",
    "Execute this code using the initial random policy, 10,000 episodes, a learning rate of 0.01 and a discount rate of 1.0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          up      down      left     right\n",
      "0   0.891326  5.510185  1.064944  4.169993\n",
      "1   0.705754  0.584034  4.825583  4.119200\n",
      "2   0.480345  0.293423  4.140278  4.161996\n",
      "3   0.582946  0.341433  4.107992  4.757389\n",
      "4   1.124187  5.525041  4.184091  1.020115\n",
      "5   4.790001  6.835361  1.344366  1.457405\n",
      "6   0.000000  0.000000  0.000000  0.000000\n",
      "7   0.000000  0.000000  0.000000  0.000000\n",
      "8   0.000000  0.000000  0.000000  0.000000\n",
      "9   4.680858  6.945766  1.466726  1.698854\n",
      "10  5.643246  5.517427  2.012642  9.875382\n",
      "11  4.737701  4.618291  2.795439  9.486641\n",
      "12  0.000000  0.000000  0.000000  0.000000\n",
      "13  3.903891  4.163002  9.726000  2.339143\n",
      "14  5.380928  5.569570  9.786805  1.511825\n",
      "15  6.792272  4.604561  1.458158  1.445315\n",
      "16  0.000000  0.000000  0.000000  0.000000\n",
      "17  0.000000  0.000000  0.000000  0.000000\n",
      "18  0.000000  0.000000  0.000000  0.000000\n",
      "19  6.718090  4.490556  1.089455  1.224381\n",
      "20  5.532605  0.887222  0.967277  4.144010\n",
      "21  0.549259  0.679608  4.665608  3.924000\n",
      "22  0.440287  0.694647  4.121145  4.047221\n",
      "23  0.592013  0.504071  3.914516  4.575904\n",
      "24  5.538231  0.707926  4.045132  0.618240\n"
     ]
    }
   ],
   "source": [
    "def update_double_Q(q1, q2, current_state, a_index, reward, alpha, gamma):\n",
    "    \"\"\"Function to update the actions values in the Q matrix\"\"\"\n",
    "    ## Get s_prime given s and a\n",
    "    s_prime, reward_prime, terminal = Q_simulate_environment(current_state, Q_action_lookup(a_index))\n",
    "    a_prime_index = nr.choice(np.where(reward_prime == max(reward_prime))[0], size = 1)[0]\n",
    "    ## Update the action values \n",
    "    q1[current_state,a_index] = q1[current_state,a_index] + alpha * (reward + gamma * (q2[s_prime,a_prime_index] - q1[current_state,a_index]))\n",
    "    return q1, s_prime, reward_prime, terminal, a_prime_index\n",
    "\n",
    "\n",
    "def double_Q_learning_0(policy, episodes, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function to perform Q-learning(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    \n",
    "    ## Initialize both Q matricies\n",
    "    Q1 = np.zeros((n_states,n_actions))\n",
    "    Q2 = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for (i) in range(episodes): # Loop over the episodes\n",
    "        #print('Executing episode number ' + str(i))\n",
    "        terminal = False\n",
    "        ## Find the inital state, action index and reward\n",
    "        current_state, a_index, reward = Q_start_episode(n_states,n_actions)\n",
    "        \n",
    "        while(not terminal): # Episode ends where get to terminal state   \n",
    "            ## Update the action values in Q1 or Q2 based on random choice\n",
    "            if(nr.uniform() <= 0.5):\n",
    "                Q1, s_prime, reward_prime, terminal, a_prime_index = update_double_Q(Q1, Q2, current_state, a_index, reward, alpha, gamma)\n",
    "            else:\n",
    "                Q2, s_prime, reward_prime, terminal, a_prime_index = update_double_Q(Q2, Q1, current_state, a_index, reward, alpha, gamma)\n",
    "            #print('s_prime: ' + str(s_prime) + ', terminal: ' + str(terminal) + ', a_prime_index: ' + str(a_prime_index))\n",
    "            ## Set action, reward and state for next iteration\n",
    "            a_index = a_prime_index\n",
    "            current_state = s_prime\n",
    "            reward = reward_prime[a_prime_index]\n",
    "    return(Q1)\n",
    "\n",
    "Q = double_Q_learning_0(initial_policy, 10000, 0.01, 1.0)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the action values of the taboo states and the terminal state are  all 0.\n",
    "\n",
    "With the action values computed using the action values, you will now find an improved policy. Make sure you create a copy of the initial random policy with `copy.deepcopy`. Execute the `update_policy` function using the copy of the initial random policy, the action value array you computed, and with $\\epsilon = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 1: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 2: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 3: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 4: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 5: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 9: {'u': 0.01, 'd': 0.97, 'l': 0.01, 'r': 0.01},\n",
       " 10: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 11: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 13: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 14: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 15: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 19: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 20: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01},\n",
       " 21: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 22: {'u': 0.01, 'd': 0.01, 'l': 0.97, 'r': 0.01},\n",
       " 23: {'u': 0.01, 'd': 0.01, 'l': 0.01, 'r': 0.97},\n",
       " 24: {'u': 0.97, 'd': 0.01, 'l': 0.01, 'r': 0.01}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = deepcopy(initial_policy)\n",
    "policy = update_policy(policy, Q, 0.01)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will evaluate your policy using the MC algorithm. In the cells below execute the required function with 10,000 episodes and display the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3196559 , 1.83115523, 1.51742799, 1.83448155, 2.32018978,\n",
       "       3.15234832, 0.        , 0.        , 0.        , 3.14913269,\n",
       "       4.74634563, 9.65036496, 0.        , 9.65021637, 4.74956785,\n",
       "       3.14029221, 0.        , 0.        , 0.        , 3.13802   ,\n",
       "       2.32839008, 1.82910856, 1.50188568, 1.81944747, 2.31784897])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_state_values(policy, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these state values reasonably similar to those found with the SARSA and is this what you expect and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS 1: Yes, these values are very similar and this is expected since both algorithms yield unbaised results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
